{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lists of quarterly transactions can be downloaded from the SEC. This script contains the following data preprocessing steps for a single year:**\n",
    "\n",
    "1. Merging data on non-derivative transactions with data on the trader's name and their corresponding stock ticker (issuer) for each quarter\n",
    "2. Concatenating all quarters\n",
    "3. Remove trades that were made by institutional or corporate entities (incl. all transactions made by LLCs, LPs, and INCs)\n",
    "4. Group-based imputation of missing titles\n",
    "5. Convert TRANS_DATE column to datetime format\n",
    "6. Exporting the filtered dataset with all trades for a given year as a Parquet file\n",
    "\n",
    "Link to raw data: https://www.sec.gov/data-research/sec-markets-data/insider-transactions-data-sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n5/_f2xjmkj39gb7s0th_6lyk9w0000gn/T/ipykernel_48903/3221235305.py:4: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nonderiv_trans_df2024q4 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/NONDERIV_TRANS.tsv\", sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# 2024q4\n",
    "\n",
    "reportingowner_df2024q4 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/REPORTINGOWNER.tsv\", sep='\\t')\n",
    "nonderiv_trans_df2024q4 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/NONDERIV_TRANS.tsv\", sep='\\t')\n",
    "submission_df2024q4 = pd.read_csv(\"//Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/SUBMISSION.tsv\", sep='\\t')\n",
    "\n",
    "# Merge transactions with reporting owner info (now with additional columns)\n",
    "nonderiv_with_owner2024q4 = nonderiv_trans_df2024q4.merge(\n",
    "    reportingowner_df2024q4[['ACCESSION_NUMBER', 'RPTOWNERNAME', 'RPTOWNER_RELATIONSHIP', 'RPTOWNER_TITLE']],\n",
    "    on='ACCESSION_NUMBER',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with submission info to add issuer name and ticker\n",
    "nonderiv_with_owner2024q4 = nonderiv_with_owner2024q4.merge(\n",
    "    submission_df2024q4[['ACCESSION_NUMBER', 'ISSUERNAME', 'ISSUERTRADINGSYMBOL']],\n",
    "    on='ACCESSION_NUMBER',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n5/_f2xjmkj39gb7s0th_6lyk9w0000gn/T/ipykernel_48903/2506553778.py:5: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nonderiv_trans_df2024q3 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/NONDERIV_TRANS.tsv\", sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# 2024q3\n",
    "\n",
    "# Load all relevant tables into pandas DataFrames\n",
    "reportingowner_df2024q3 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/REPORTINGOWNER.tsv\", sep='\\t')\n",
    "nonderiv_trans_df2024q3 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/NONDERIV_TRANS.tsv\", sep='\\t')\n",
    "submission_df2024q3 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/SUBMISSION.tsv\", sep='\\t')\n",
    "\n",
    "# Merge transactions with reporting owner info (including relationship and title)\n",
    "nonderiv_with_owner2024q3 = nonderiv_trans_df2024q3.merge(\n",
    "    reportingowner_df2024q3[['ACCESSION_NUMBER', 'RPTOWNERNAME', 'RPTOWNER_RELATIONSHIP', 'RPTOWNER_TITLE']],\n",
    "    on='ACCESSION_NUMBER',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with submission info to add issuer name and ticker\n",
    "nonderiv_with_owner2024q3 = nonderiv_with_owner2024q3.merge(\n",
    "    submission_df2024q3[['ACCESSION_NUMBER', 'ISSUERNAME', 'ISSUERTRADINGSYMBOL']],\n",
    "    on='ACCESSION_NUMBER',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n5/_f2xjmkj39gb7s0th_6lyk9w0000gn/T/ipykernel_48903/1628854452.py:5: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nonderiv_trans_df2024q2 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/NONDERIV_TRANS.tsv\", sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# 2024q2\n",
    "\n",
    "# Load all relevant tables into pandas DataFrames\n",
    "reportingowner_df2024q2 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/REPORTINGOWNER.tsv\", sep='\\t')\n",
    "nonderiv_trans_df2024q2 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/NONDERIV_TRANS.tsv\", sep='\\t')\n",
    "submission_df2024q2 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/SUBMISSION.tsv\", sep='\\t')\n",
    "\n",
    "# Merge transactions with reporting owner info (including relationship and title)\n",
    "nonderiv_with_owner2024q2 = nonderiv_trans_df2024q2.merge(\n",
    "    reportingowner_df2024q2[['ACCESSION_NUMBER', 'RPTOWNERNAME', 'RPTOWNER_RELATIONSHIP', 'RPTOWNER_TITLE']],\n",
    "    on='ACCESSION_NUMBER',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with submission info to add issuer name and ticker\n",
    "nonderiv_with_owner2024q2 = nonderiv_with_owner2024q2.merge(\n",
    "    submission_df2024q2[['ACCESSION_NUMBER', 'ISSUERNAME', 'ISSUERTRADINGSYMBOL']],\n",
    "    on='ACCESSION_NUMBER',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n5/_f2xjmkj39gb7s0th_6lyk9w0000gn/T/ipykernel_48903/2066577900.py:5: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  nonderiv_trans_df2024q1 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/NONDERIV_TRANS.tsv\", sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# 2024q1\n",
    "\n",
    "# Load all relevant tables into pandas DataFrames\n",
    "reportingowner_df2024q1 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/REPORTINGOWNER.tsv\", sep='\\t')\n",
    "nonderiv_trans_df2024q1 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/NONDERIV_TRANS.tsv\", sep='\\t')\n",
    "submission_df2024q1 = pd.read_csv(\"/Users/gianjaeger/Desktop/Networks Summative Project/Data/2024q4_form345/SUBMISSION.tsv\", sep='\\t')\n",
    "\n",
    "# Merge transactions with reporting owner info (including relationship and title)\n",
    "nonderiv_with_owner2024q1 = nonderiv_trans_df2024q1.merge(\n",
    "    reportingowner_df2024q1[['ACCESSION_NUMBER', 'RPTOWNERNAME', 'RPTOWNER_RELATIONSHIP', 'RPTOWNER_TITLE']],\n",
    "    on='ACCESSION_NUMBER',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with submission info to add issuer name and ticker\n",
    "nonderiv_with_owner2024q1 = nonderiv_with_owner2024q1.merge(\n",
    "    submission_df2024q1[['ACCESSION_NUMBER', 'ISSUERNAME', 'ISSUERTRADINGSYMBOL']],\n",
    "    on='ACCESSION_NUMBER',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317928, 33)\n"
     ]
    }
   ],
   "source": [
    "# MERGE ALL FOUR\n",
    "\n",
    "# Combine all quarterly non-derivative trades into one annual dataframe\n",
    "nonderiv_with_owner_2024 = pd.concat(\n",
    "    [nonderiv_with_owner2024q1, nonderiv_with_owner2024q2,\n",
    "     nonderiv_with_owner2024q3, nonderiv_with_owner2024q4],\n",
    "    ignore_index=True  # ensures the index is reset properly\n",
    ")\n",
    "\n",
    "# Optional: Preview the result\n",
    "print(nonderiv_with_owner_2024.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 52408 rows. Remaining rows: 265520\n"
     ]
    }
   ],
   "source": [
    "# Define boolean masks for each condition\n",
    "llc_mask = nonderiv_with_owner_2024['RPTOWNERNAME'].str.contains('LLC', case=False, na=False)\n",
    "lp_mask = nonderiv_with_owner_2024['RPTOWNERNAME'].str.contains(r'\\bL\\.?P\\b', case=False, na=False, regex=True)\n",
    "starts_with_number_mask = nonderiv_with_owner_2024['RPTOWNERNAME'].str.match(r'^\\d', na=False)\n",
    "inc_mask = nonderiv_with_owner_2024['RPTOWNERNAME'].str.contains(r'\\bINC\\b', case=False, na=False)\n",
    "\n",
    "# Combine all masks using OR\n",
    "combined_mask = llc_mask | lp_mask | starts_with_number_mask | inc_mask\n",
    "\n",
    "# Remove the rows that match any of the conditions\n",
    "filtered_2024_df = nonderiv_with_owner_2024[~combined_mask].copy()\n",
    "\n",
    "# (Optional) Check how many rows were removed\n",
    "removed_count = combined_mask.sum()\n",
    "print(f\"Removed {removed_count} rows. Remaining rows: {len(filtered_2024_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete redundant columns\n",
    "\n",
    "columns_to_drop = [\n",
    "    'SECURITY_TITLE_FN',\n",
    "    'TRANS_DATE_FN',\n",
    "    'DEEMED_EXECUTION_DATE',\n",
    "    'DEEMED_EXECUTION_DATE_FN',\n",
    "    'TRANS_TIMELINESS',\n",
    "    'TRANS_TIMELINESS_FN',\n",
    "    'TRANS_ACQUIRED_DISP_CD_FN',\n",
    "    'SHRS_OWND_FOLWNG_TRANS_FN',\n",
    "    'VALU_OWND_FOLWNG_TRANS',\n",
    "    'VALU_OWND_FOLWNG_TRANS_FN',\n",
    "    'DIRECT_INDIRECT_OWNERSHIP_FN'\n",
    "]\n",
    "\n",
    "filtered_2024_df = filtered_2024_df.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n5/_f2xjmkj39gb7s0th_6lyk9w0000gn/T/ipykernel_48903/2320419102.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  filtered_2024_df['RPTOWNER_TITLE'] = filtered_2024_df.groupby('RPTOWNERNAME')['RPTOWNER_TITLE'].transform(lambda x: x.ffill().bfill())\n"
     ]
    }
   ],
   "source": [
    "# Fill missing RPTOWNER_TITLE values using known values for the same RPTOWNERNAME\n",
    "filtered_2024_df['RPTOWNER_TITLE'] = filtered_2024_df.groupby('RPTOWNERNAME')['RPTOWNER_TITLE'].transform(lambda x: x.ffill().bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANS_DATE is datetime: False\n"
     ]
    }
   ],
   "source": [
    "# Check if TRANS_DATE is datetime dtype\n",
    "is_datetime = pd.api.types.is_datetime64_any_dtype(filtered_2024_df['TRANS_DATE'])\n",
    "\n",
    "print(f\"TRANS_DATE is datetime: {is_datetime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 4 rows due to invalid or missing TRANS_DATE entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n5/_f2xjmkj39gb7s0th_6lyk9w0000gn/T/ipykernel_48903/3877917237.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  filtered_2024_df['TRANS_DATE'] = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "# Convert TRANS_DATE to datetime, invalid entries will become NaT\n",
    "filtered_2024_df['TRANS_DATE'] = pd.to_datetime(\n",
    "    filtered_2024_df['TRANS_DATE'],\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Count rows before dropping\n",
    "initial_count = len(filtered_2024_df)\n",
    "\n",
    "# Drop rows with NaT in TRANS_DATE\n",
    "filtered_2024_df = filtered_2024_df.dropna(subset=['TRANS_DATE'])\n",
    "\n",
    "# Count rows after dropping\n",
    "final_count = len(filtered_2024_df)\n",
    "\n",
    "# Report how many were deleted\n",
    "deleted_count = initial_count - final_count\n",
    "print(f\"Deleted {deleted_count} rows due to invalid or missing TRANS_DATE entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter for rows where TRANS_DATE is in 2024\n",
    "# filtered_2024_df = filtered_2024_df[\n",
    "#     filtered_2024_df['TRANS_DATE'].dt.year == 2024\n",
    "# ].copy()\n",
    "\n",
    "# # Optional: Check how many rows remain\n",
    "# print(f\"Remaining rows from 2024: {len(filtered_2024_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final dataframe for each year\n",
    "\n",
    "filtered_2024_df.to_parquet(\"/Users/gianjaeger/Downloads/filtered_2024_df.parquet\", index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine all years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to folder containing parquet files\n",
    "folder_path = \"/Users/gianjaeger/Desktop/Networks Summative Project/Preprocessed Data (trades by year)\"\n",
    "\n",
    "# Years to import\n",
    "years = range(2014, 2024)\n",
    "\n",
    "# Dictionary to hold each year's dataframe\n",
    "df_dict = {}\n",
    "\n",
    "for year in years:\n",
    "    file_path = os.path.join(folder_path, f\"filtered_{year}_df.parquet\")\n",
    "    df_dict[year] = pd.read_parquet(file_path)\n",
    "\n",
    "# Optional: Print to confirm\n",
    "for year, df in df_dict.items():\n",
    "    print(f\"{year}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all dataframes into one\n",
    "all_trades_df = pd.concat(df_dict.values(), ignore_index=True)\n",
    "\n",
    "# Optional: Check final shape\n",
    "print(f\"Combined DataFrame shape: {all_trades_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows before filtering\n",
    "initial_count = len(all_trades_df)\n",
    "\n",
    "# Filter: keep only rows where TRANS_DATE is on or after Jan 1, 2014\n",
    "all_trades_df = all_trades_df[all_trades_df['TRANS_DATE'] >= pd.Timestamp('2014-01-01')]\n",
    "\n",
    "# Count rows after filtering\n",
    "final_count = len(all_trades_df)\n",
    "\n",
    "# Calculate how many were removed\n",
    "removed_count = initial_count - final_count\n",
    "print(f\"Removed {removed_count} trades that occurred before 2014.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save intermediate dataframe\n",
    "all_trades_df.to_parquet(\"/Users/gianjaeger/Downloads/all_trades_df.parquet\", index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove trades that are somehow newer than 2024 (<30 trades) and trades classified as form 5 (roughly 100,000 trdaes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trades_df = all_trades_df[all_trades_df['TRANS_FORM_TYPE'] != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure TRANS_DATE is in datetime format\n",
    "all_trades_df['TRANS_DATE'] = pd.to_datetime(all_trades_df['TRANS_DATE'])\n",
    "\n",
    "# Filter to keep only rows with year <= 2024\n",
    "all_trades_df = all_trades_df[all_trades_df['TRANS_DATE'].dt.year <= 2024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove all trades that are form 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trades = all_trades[all_trades['TRANS_FORM_TYPE'] != 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group based imputing for missing titles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all names (Note that this step will reduce the distinct number of names by nearly 100,000)\n",
    "all_trades['RPTOWNERNAME_lower'] = all_trades['RPTOWNERNAME'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove additional institutional trades and convert to CSV with key columns to be used in C++**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trades = pd.read_parquet(\"/Users/gianjaeger/Desktop/Networks Summative Project/All trades (use for replication)/all_trades.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rows. Remaining rows: 2738328\n"
     ]
    }
   ],
   "source": [
    "# List of unwanted terms\n",
    "unwanted_terms = [\n",
    "    'blackstone', 'l.l.c', 'sequoia', 'kkr', 'llp', 'ltd', 'fund', 'bcp',\n",
    "    'sicav', 'associates', 'partnership', 'l p', 'corp', 'trust', 'partners', 'co.'\n",
    "]\n",
    "\n",
    "# Create a regex pattern to match any unwanted term (whole words, case-insensitive)\n",
    "pattern = '|'.join([rf'\\b{term}\\b' for term in unwanted_terms])\n",
    "\n",
    "# Identify rows to remove\n",
    "unwanted_mask = all_trades['RPTOWNERNAME_lower'].str.contains(pattern, case=False, na=False, regex=True)\n",
    "\n",
    "# Remove unwanted rows in-place\n",
    "all_trades = all_trades[~unwanted_mask]\n",
    "\n",
    "# (Optional) Check how many rows were removed\n",
    "print(f\"Removed {unwanted_mask.sum()} rows. Remaining rows: {len(all_trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rows based on additional terms. Remaining rows: 2738328\n"
     ]
    }
   ],
   "source": [
    "# List of new unwanted terms\n",
    "additional_unwanted_terms = ['holdings', 'international', 'management', 'b.v', 'bank']\n",
    "\n",
    "# Create regex pattern\n",
    "additional_pattern = '|'.join([rf'\\b{term}\\b' for term in additional_unwanted_terms])\n",
    "\n",
    "# Create mask for additional unwanted rows\n",
    "additional_mask = all_trades['RPTOWNERNAME_lower'].str.contains(additional_pattern, case=False, na=False, regex=True)\n",
    "\n",
    "# Remove the rows\n",
    "all_trades = all_trades[~additional_mask]\n",
    "\n",
    "# (Optional) Print summary\n",
    "print(f\"Removed {additional_mask.sum()} rows based on additional terms. Remaining rows: {len(all_trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rows ending with 'co'. Remaining rows: 2738328\n"
     ]
    }
   ],
   "source": [
    "# Create mask for rows ending with 'co' (case-insensitive)\n",
    "ends_with_co_mask = all_trades['RPTOWNERNAME_lower'].str.strip().str.endswith(' co.', na=False)\n",
    "\n",
    "# Remove the rows\n",
    "all_trades = all_trades[~ends_with_co_mask]\n",
    "\n",
    "# (Optional) Print summary\n",
    "print(f\"Removed {ends_with_co_mask.sum()} rows ending with 'co'. Remaining rows: {len(all_trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rows based on final terms. Remaining rows: 2738328\n"
     ]
    }
   ],
   "source": [
    "# Updated list of final unwanted terms\n",
    "final_unwanted_terms = [\n",
    "    'axa', 's.a', 'holding', 'investment', 'group', 'international', 'insurance'\n",
    "]\n",
    "\n",
    "# Create regex pattern\n",
    "final_pattern = '|'.join([rf'\\b{term}\\b' for term in final_unwanted_terms])\n",
    "\n",
    "# Create mask for rows to remove\n",
    "final_mask = all_trades['RPTOWNERNAME_lower'].str.contains(final_pattern, case=False, na=False, regex=True)\n",
    "\n",
    "# Remove the rows\n",
    "all_trades = all_trades[~final_mask]\n",
    "\n",
    "# (Optional) Print summary\n",
    "print(f\"Removed {final_mask.sum()} rows based on final terms. Remaining rows: {len(all_trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows after removing missing symbols: 2735932\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where ISSUERTRADINGSYMBOL is missing or empty\n",
    "all_trades = all_trades[all_trades['ISSUERTRADINGSYMBOL'].notna() & (all_trades['ISSUERTRADINGSYMBOL'].str.strip() != '')]\n",
    "\n",
    "# (Optional) Print how many rows remain\n",
    "print(f\"Remaining rows after removing missing symbols: {len(all_trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DataFrame created with shape: (2735932, 4)\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to keep\n",
    "columns_to_keep = [\n",
    "    'ISSUERTRADINGSYMBOL',\n",
    "     'RPTOWNERNAME_lower',\n",
    "    'TRANS_ACQUIRED_DISP_CD',\n",
    "    'TRANS_DATE'\n",
    "]\n",
    "\n",
    "# Create the new filtered DataFrame\n",
    "all_trades_cleaned = all_trades[columns_to_keep].copy()\n",
    "\n",
    "# (Optional) Check result\n",
    "print(f\"New DataFrame created with shape: {all_trades_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to: /Users/gianjaeger/Downloads/all_trades_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Save final dataframe as CSV to use in C++\n",
    "\n",
    "# Define the path to your Downloads folder\n",
    "downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "\n",
    "# Define the full file path\n",
    "csv_file_path = os.path.join(downloads_path, \"all_trades_cleaned.csv\")\n",
    "\n",
    "# Save the DataFrame as CSV\n",
    "all_trades_cleaned.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"CSV file saved to: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
